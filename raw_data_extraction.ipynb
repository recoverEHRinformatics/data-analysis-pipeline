{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if any of the modules above are not already installed please use the command below in your notebook to install the module\n",
    "# !pip install NameOfYourModule (e.g. !pip install pandas)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import text as sqlalchemy_text\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the empty strings below with the correct server/database information\n",
    "server = \"\"\n",
    "database = \"\"\n",
    "username = \"\"\n",
    "password = \"\"\n",
    "port = ''\n",
    "\n",
    "database_string = f\"postgres+psycopg2://{username}:{password}@{server}:{port}/{database}\"\n",
    "\n",
    "database_engine = create_engine(database_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main folder of the query\n",
    "main_path = \"\"\n",
    "\n",
    "# where the raw data will be saved\n",
    "source_data_path = f\"{main_path}/source data\"\n",
    "# create an empty folder if it does not exist\n",
    "if os.path.exists(source_data_path) != True:\n",
    "    os.makedirs(source_data_path)\n",
    "\n",
    "# where the results will be saved\n",
    "result_path = f\"{main_path}/result\"\n",
    "# create an empty folder if it does not exist\n",
    "if os.path.exists(result_path) != True:\n",
    "    os.makedirs(result_path)\n",
    "\n",
    "# where all external data needed for analysis is already saved (e.g. PASC definition spreadsheet)\n",
    "external_source_path = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of site names used in the analysis\n",
    "# the site names should exactly match the schema names live in the database\n",
    "site_names = ['mshs', 'wcm', 'nyu', 'montefiore', 'columbia']\n",
    "\n",
    "# Study period start and end date (YYYY-MM-DD)\n",
    "study_start_date = '2020-03-01'\n",
    "study_end_date = '2022-07-30'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_data(query: str, site_names: list, source_data_path: str, data_name: str, database_engine: str):\n",
    "    '''extract_raw_data is a function to query the live data for all sites in the analysis, concatenate them together, and save them as parquet files.\n",
    "\n",
    "    Args:\n",
    "        query (str): SQL query to be executed aginst the live data in database.\n",
    "        site_names (list): a list of all site names (schemas as they appear in the database) used in the analysis.\n",
    "        source_data_path (str): the source data folder path where the final data frame will be saved.\n",
    "        data_name (str): name of the table as it appears in the database.\n",
    "        database_engine (str): the database engine address.\n",
    "\n",
    "    Returns:\n",
    "        final_df (DataFrame): a pandas dataframe containing all sites data\n",
    "    '''\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    counter = 1\n",
    "\n",
    "    for site in site_names:\n",
    "        try:\n",
    "            print(f\"Query {counter}. {site}'s {data_name} started\")\n",
    "\n",
    "            # replace where input query indicates \"<SCHEMA>\" with the site's real schema\n",
    "            modified_query = query.replace(\"<SCHEMA>\", f\"{site}\")\n",
    "\n",
    "            df = pd.read_sql(\n",
    "                modified_query, database_engine\n",
    "            )\n",
    "            # creating an additional column to indicate the site\n",
    "            df['site'] = site\n",
    "            print(f\"{site}'s {data_name} query is finished\")\n",
    "\n",
    "            # optional lines to generate information about each site's data\n",
    "            print(f\"{site} table shape: {df.shape}\")\n",
    "            print(f\"{site} table has: {len(df.syn_pt_id.unique())} unique patients\")\n",
    "\n",
    "            # concatenate individual site data into one data frame (i.e. final_df)\n",
    "            final_df = pd.concat([final_df, df], ignore_index=True)\n",
    "\n",
    "            del df\n",
    "            counter += 1\n",
    "            print(\"*\"*50)\n",
    "\n",
    "        # error-agnostic pass which will leave out the individual site\n",
    "        # make sure to investigate further why the query failed for a specific site\n",
    "        # best way to investigate the issue is to run the SQL query in PgAdmin\n",
    "        # possible issues could be data type mismatch for certain columns\n",
    "        except:\n",
    "            error_msg = f\"# {counter}. {site}'s {data_name} WAS NOT PROCESSED #\"\n",
    "            print(\"#\"*len(error_msg))\n",
    "            print(error_msg)\n",
    "            print(\"#\"*len(error_msg))\n",
    "\n",
    "    # saving the table with all sites data concatenated as parquet format in source data folder\n",
    "    pq.write_table(pa.Table.from_pandas(\n",
    "        final_df), f\"{source_data_path}/{data_name}.parquet\", compression=\"BROTLI\")\n",
    "    print(f\"All sites {data_name} data have been saved as a parquet file in:\")\n",
    "    print(f\"{source_data_path}/{data_name}.parquet\")\n",
    "\n",
    "    # optional lines to generate information about all sites data\n",
    "    print(f\"{site} table shape: {final_df.shape}\")\n",
    "    print(f\"{site} table has: {len(final_df.syn_pt_id.unique())} unique patients\")\n",
    "    print(f\"{site} table has: {len(final_df.site.unique())} unique sites\")\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to replace all mentions of the site's schema name in the SQL query below with <SCHEMA>\n",
    "# extract_raw_data function will automatically replace all <SCHEMA> instaces with the site's name when executing the query\n",
    "# make sure to always include the first line after SELECT statement to generate a unique synthetic patient identifier (i.e. syn_pt_id)\n",
    "# the unique synthetic patient identifier (i.e. syn_pt_id) ensures there are no overlapping patid across sites\n",
    "# make sure to only select columns needed for analysis in the SELECT statement and avoid using \"*\"\n",
    "test_query = f\"\"\"\n",
    "SELECT\n",
    "CONCAT({\"'<SCHEMA>'\"}, '_', patid) AS syn_pt_id\n",
    ", t1.* \n",
    "FROM <SCHEMA>.demographic t1\n",
    "LIMIT 100;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic = extract_raw_data(\n",
    "    query=test_query, \n",
    "    site_names=site_names, \n",
    "    source_data_path=source_data_path, \n",
    "    data_name='demographic_test',\n",
    "    database_engine=database_engine)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
